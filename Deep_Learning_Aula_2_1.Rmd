---
title: "Deep Learning - aula 2"
---

# Introdução

### Regressão Linear

#### Vamos usar o data set - Boston Housing Values in Suburbs of Boston

crim - per capita crime rate by town. \
zn - proportion of residential land zoned for lots over 25,000 sq.ft. \
indus - proportion of non-retail business acres per town. \
chas - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise). \
nox - nitrogen oxides concentration (parts per 10 million). \
rm - average number of rooms per dwelling. \
age - proportion of owner-occupied units built prior to 1940. \
dis - weighted mean of distances to five Boston employment centres. \
rad - index of accessibility to radial highways. \
tax - full-value property-tax rate per \$10,000. \
ptratio - pupil-teacher ratio by town. \
black - 1000(Bk − 0.63)2 where Bk is the proportion of blacks by town. \
lstat - lower status of the population (percent). \
medv - median value of owner-occupied homes in \$1000s. \


```{r}

# Conceito de função
# y = f(x)
# y = a +b1*x1 +b2*x2...

# install.packages("MASS")
library("MASS")
library("rpart")


set.seed(0)


```

```{r}
# Baixa os dados
data <- Boston

# Uma olhada nos dados
head(data)



```

```{r}
# Temos valores nulos?
data[is.na(data) == TRUE]

# Train-Test Split
train_test_split_index <- 0.8 * nrow(data)

train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])


```

```{r}
# --
# CART

# árvore
# ?rpart - Recursive Partitioning and Regression Trees

fit_tree <- rpart(medv ~., method = "anova", data=train) # anova - regressão (definido pelo tipo da variável Y)
tree_predict <- predict(fit_tree, test)                  # predict função do pacote statis
mse_tree <- mean((tree_predict - test$medv) ^ 2)         # calcular o Erro Quadrático Médio "MSE"

cat("MSE:", mse_tree)
```

```{r}
fit_tree$frame

```


### NeuralNet

```{r}

# Padronizar dados para melhor performance
# Explicar apply

set.seed(0)

```

```{r}
max_data <- apply(data, 2, max) 
min_data <- apply(data, 2, min)

scaled <- scale(data, center = min_data, scale = max_data - min_data)

```

```{r}

#?sample

index = sample(1:nrow(data), round(0.70 * nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])


```

```{r}
library(neuralnet)

# abrir o CRAN para mostrar

# Fit de neuralnet
# Executar testes com diferentes arquiteturas 

# hidden -> numero de camadas e neuronios
#?neuralnet

nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,data=train_data,hidden=c(5,4,3,2),linear.output=T)
plot(nn)


```


```{r}
# voltar com valor não normalizado para comparação

pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE_nn <- mean((pr.nn_ - test.r)^2)

cat("MSE:", MSE_nn)
```


#Plot
```{r}

plot(test_data$medv,type = 'l',col="red",xlab = "x", ylab = "Valor Residencia")
lines(pr.nn$net.result,col = "blue")

```


# Exercício 1
```{r}

library(ISLR)
library(neuralnet)


# Olhar os dados - data wrangling?
data <- College
# is.na(data)
head(data)

```

```{r}

# private = as.numeric(College$Private)-1
private <- ifelse(data$Private == 'Yes', 1, 0)


```

```{r}
# Padronizar dados para melhor performance
data <- data[,2:18]
max_data <- apply(data, 2, max) 
min_data <- apply(data, 2, min)
scaled <- data.frame(scale(data,center = min_data, scale = max_data - min_data))


```

```{r}
# Inclui variável explicada (target)
scaled$Private <- private


set.seed(0)

# train test split
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])


```

```{r}
# Utiliza o neuralnet
set.seed(0)
n = names(train_data)

f <- as.formula(paste("Private ~", paste(n[!n %in% "Private"], collapse = " + "))) # cria uma string com a fórmula

nn <- neuralnet(f,data=train_data,hidden=c(5,4),linear.output=F)                   # não temos um output linear, mas classificatório
plot(nn)

pr.nn <- compute(nn,test_data[,1:17])


```

```{r}
# Explica sapply
pr.nn$net.result <- sapply(pr.nn$net.result, round, digits=0)
#pr.nn$net.result

table(test_data$Private, pr.nn$net.result)

Acc <- (61+152) / (61+152+8+12)



```

```{r}

# CART comparação
set.seed(0)

# árvore
fit_tree <- rpart(f,method="class", data=train_data)
tree_predict <- predict(fit_tree, test_data, type = "class")
table(test_data$Private, tree_predict)

Acc_tree <- (58+159) / (58+159+11+5)
```

