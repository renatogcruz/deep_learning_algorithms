---
title: "R Notebook"
output: Aula 3 - Deep Learning
---

# Pacotes

```{r}

library("rattle")
library("rnn")
library("ggplot2")

```

RNN - Rede Neural Recorrente

- Isso significa que temos uma rede que "guarda" todo o passado;
- Explicar crescimento econônimoco com base em nível de confiança;
- Simples Recurrent Neural Network.

Rede de Elman

- Resultado que os valores da camada escondida estarão multiplicados por valores de pesos elevados a potencias maiores quanto mais antiga for essa informação.

Backpropagation

Dissipação do Gradiente

- intuição;
- Multiplicação de números menores do que 1;
- Multiplicação de números maiores do que 1.

Problemas com Gradiente

- Ocorre em qualquer rede - mais comum em RNN;
- Quanto mais profunda, pior

Backpropagation Truncada

- O processo truncado consiste em parar a avaliação de mudanças de pesos até certo ponto.
- Custo computcional;
- Solução arbitrária.

REsolver Vanishing Gradient

- Inicilização de matriz de peso;
- Função de ativação ReLU.

Clipping do Gradiente

- Solução possível para Dissipação e Explosão
- O clipping define um valor limite definido nos gradientes, o que significa que, mesmo se um gradiente aumentar além do valor predefinido durante o treinamento, seu valor ainda será limitado ao limite definido. Dessa forma, a direção do gradiente permanece inalterada e apenas a magnitude do gradiente é alterada.

LSTM 

- Ligação da entrada com cada escondida - igual;
- Estado da célula- momória longa;
- forget gate, input gate, output gate.

GRU - Gate Recurrent Unit

- Resolve problema da dissipação do gradiente;
- Baseado em portões: reiniciação e atualização;
- Apenas 1 estado oculto;
- Guarda dependências longas;
- Reset Gate: Quanto de informações anteriores iremos ignorar;
- Update Gate: Qunato de informação anteior iremos manter. 

Extra: Transformadores

- Maior parte de PLN era feito com RNN;
- Attention is all you need (artigo);
- Uso de RNN -> perde informação conforme se distância do início de uma série;
- Contexto é essencial em PLN

# Dados = Weather AUS

```{r}

data(weatherAUS)
head(weatherAUS)

```

```{r}
# View(weatherAUS)
# extrair somente colunas 1 and 14  e primeiras 3040 linhas (Albury location)

data <- weatherAUS[1:3040,14:15]

summary(data)
```


# Pre-processamento

```{r}

data_cleaned <- na.omit(data)
data_used = data_cleaned[1:3000,]
x=data_used[,1]
y=data_used[,2]

# --
# minmaxscale
Yscaled = (y - min(y)) / (max(y) - min(y)) 
Xscaled = (x - min(x)) / (max(x) - min(x))

y <- Yscaled
x <- Xscaled

x <- as.matrix(x)
y <- as.matrix(y)

X <- matrix(x, nrow = 30) #100 colunas com 30 observações cada
Y <- matrix(y, nrow = 30) #100 colunas com 30 observações cada

#train test split
train=1:80
test=81:100
```

# modelo

```{r}
set.seed(12)

model <- trainr(Y = Y[, train],
                X = X[, train],
                learningrate = 0.01,  # taxa de aprendizado a ser aplicada para iteração de peso - taxa de apredizado
                hidden_dim = 15,      # dimensão(ões) da(s) camada(s) oculta(s)
                network_type = "rnn", # tipo de rede, pode ser rnn, gru ou lstm. gru e lstm são experimentais
                numepochs = 100)      # número de iteração, ou seja, número de vezes que todo o conjunto de dados é apresentado à rede


#model$error

# poucas épocas?
plot(colMeans(model$error),type='l',xlab='epoch',ylab='errors')
```

# predição

```{r}
Yp <- predictr(model, X[,test])

#Percentual de variação em uma variável explicada por outra
#por enquanto: entenda que é um percentual de variação explicada

rsq <- function(y_actual,y_predict)
{
  cor(y_actual,y_predict)^2
}


Ytest <- matrix(Y[,test], nrow = 1)
Ytest <- t(Ytest)
Ypredicted <- matrix(Yp, nrow = 1)
Ypredicted <- t(Ypredicted)

result_data <- data.frame(Ytest)
result_data$Ypredicted <- Ypredicted     

rsq <- rsq(result_data$Ytest,result_data$Ypredicted)

mtest <- mean(result_data$Ytest)
mpredicted <- mean(result_data$Ypredicted)

print(paste("rsq", rsq, "mean test", mtest, "mean predict", mpredicted))
```

# gráfico

```{r}
plot(as.vector(t(result_data$Ytest)), col = 'red', type='l',
main = "Actual vs Predicted Humidity: testing set",
ylab = "Y,Yp")
lines(as.vector(t(Yp)), type = 'l', col = 'black')
legend("bottomright", c("Predicted", "Actual"),
col = c("red","black"),
lty = c(1,1), lwd = c(1,1))
```

# EXERCICIO 1

```{r}
library("rnn")
library("dplyr")

data <- read.csv("PETR4.SA.csv")

#Inverter a ordem das ações para pegar da última para a ´primeira
data <- data[order(data$Date, decreasing = TRUE),]

fechamento <- data$Close

# Encontre os valores "anteriores" (lag()) ou "próximos" (lead()) em um vetor. 
# Útil para comparar valores anteriores ou posteriores aos valores atuais.
fechamento_anterior <- lead(fechamento,n=1L) #?lead

data_analise <- data.frame(fechamento)
data_analise$fechamento_anterior <- fechamento_anterior


summary(data_analise)


```


```{r}
#exclui NA
data_analise <- data_analise[1:248,]


x <- data_analise[,2]
y <- data_analise[,1]


X <- matrix(x, nrow = 31)
Y <- matrix(y, nrow = 31)


Yscaled <- (Y - min(Y)) / (max(Y) - min(Y))
Xscaled <- (X - min(X)) / (max(X) - min(X))
Y <- Yscaled
X <- Xscaled


train=1:6
test=7:8

set.seed(12)
model <- trainr(Y = Y[,train],
                X = X[,train],
                learningrate = 0.05,
                hidden_dim = 20,
                numepochs = 1000,
                network_type = "rnn"
                )


plot(colMeans(model$error),type='l',xlab='epoch',ylab='errors')
```

```{r}
#no conjunto de treinamento
Ytrain <- t(matrix(predictr(model, X[,train]),nrow=1))
Yreal <- t(matrix(Y[,train],nrow=1))


```

```{r}
#Percentual de variação em uma variável explicada por outra
rsq <- function(y_actual,y_predict){
  cor(y_actual,y_predict)^2
}

rsq(Yreal,Ytrain)

plot(Ytrain, type = "l", col = "darkred")
lines(Yreal, col = "darkblue", type = "l")


```

```{r}
#no conjunto de teste
Ytest=matrix(Y[,test], nrow = 1)
Ytest = t(Ytest)
Yp <- predictr(model, Y[,test])
Ypredicted=matrix(Yp, nrow = 1)
Ypredicted=t(Ypredicted)

result_data <- data.frame(Ytest)
result_data$Ypredicted <- Ypredicted     

rsq(result_data$Ytest,result_data$Ypredicted)

mean(result_data$Ytest)
mean(result_data$Ypredicted)
```




