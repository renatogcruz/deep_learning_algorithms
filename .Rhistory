# Ŷ = ∑(yk / pi)
# Tamanho da amostra total
n <- length(y1)
# Estimativa do total populacional (Y)
total_estimado <- sum(y1 / Zi) / n
# Estimativa da variância do estimador
var_estimador <- sum(((y1 / Zi) - total_estimado) ^ 2)
total_estimado
var_estimador
y1 <- c(45, 20, 60)
Zi <- c(0.02, 0.01, 0.03)
# Ŷ = ∑(yk / pi)
# Tamanho da amostra total
n <- length(y1)
# Estimativa do total populacional (Y)
total_estimado <- sum(y1 / Zi) / n
# Estimativa da variância do estimador
var_estimador <- sum(((y1 / Zi) - total_estimado) ^ 2) * ( 1 / n * (n - 1))
total_estimado
var_estimador
y1 <- c(45, 20, 60)
Zi <- c(0.02, 0.01, 0.03)
# Ŷ = ∑(yk / pi)
# Tamanho da amostra total
n <- length(y1)
# Estimativa do total populacional (Y)
total_estimado <- sum(y1 / Zi) / n
# Estimativa da variância do estimador
var_estimador <- sum(((y1 / Zi) - total_estimado) ^ 2) * ( 1 / (n * (n - 1)))
total_estimado
var_estimador
require("survey")
require("survey")
setwd("C:/Users/Renato/OneDrive/github/especializacao_estatistica_aplicada/open_applied_stat/7_amostragem/tp_final")
dados=read.table("questao_04.csv",dec=",",sep=";",header=T)
dados
Wh = 0.36
peso <- 1/Wh
fpc.es <- dados$N
des.es <- svydesign(id=~1,strata=~C,data=dados, weight=pesofpc=~N)
N=240
n=40
des.es <- svydesign(id=~1,strata=~C,data=dados, weight=pesofpc=~N)
weight <- 1/Wh
fpc.es <- dados$N
des.es <- svydesign(id=~1,strata=~C,data=dados, weight=weight, fpc=~N)
cat("Resultados\nEstimativas\n\n")
cat("\nMédias -- Estimativa -- Desvio-Padrão\n")
svymean(~y, des.es, deff="replace")
cat("\nMédias -- Intervalo de confiança\n")
confint(svymean(~y,design=des.es),level=0.95)
cat("\nTotal -- Estimativa -- Desvio-Padrão\n")
svytotal(~y,des.es)
cat("\nTotal -- Intervalo de confiança\n")
confint(svytotal(~y,design=des.es),level=0.95)
pacotes <- c("plotly","tidyverse")
pacotes <- c("plotly",
"tidyverse")
if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
instalador <- pacotes[!pacotes %in% installed.packages()]
for(i in 1:length(instalador)) {
install.packages(instalador, dependencies = T)
break()}
sapply(pacotes, require, character = T)
} else {
sapply(pacotes, require, character = T)
}
#### Garante que os resultados sejam reproduzíveis
set.seed(0)
#### Lê nossos dados de vinho
data <- read.csv(file = "winequality-red.csv")
data
data <- scale(data)
#### Estatísticas univariadas
summary(data)
nrow(data)
View(data)
0.8 * nrow(data)
train_test_split_index
#### Train Test Split (total de linhas = 1599 | 80% = 1279.2)
train_test_split_index <- 0.8 * nrow(data)
train_test_split_index # output: [1] 1279.2 (valor float)
data[1:train_test_split_index,]
data.frame(data[1:train_test_split_index,])
# conjunto de treinamento
train <- data.frame(data[1:train_test_split_index,])
data.frame(data[(train_test_split_index+1): nrow(data),])
# conjunto de teste
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
data.frame(train[1:11])
data.frame(train[12])
#### Padronizar dados para melhor performance
train_x <- data.frame(train[1:11]) # variáveis explicativas (xs)
train_y <- data.frame(train[12])   # variável target (y)
test_x <- data.frame(test[1:11])   # variáveis explicativas (xs)
test_y <- data.frame(test[12])     # variável target (y)
t(train_x)
#### transposição da matriz para facilitar
train_x <- t(train_x)
train_y <- t(train_y)
test_x <- t(test_x )
test_y <- t(test_y)
getLayerSize <- function(X, y, hidden_neurons) {
n_x <- dim(X)[1]      # quantidade de linhas de x = neurônios da camada de entrada
n_h <- hidden_neurons # quantidade de neurônios na camada escondida
n_y <- dim(y)[1]      # quantidade de linhas de y = neurônios da camada de saída
size <- list("n_x" = n_x,
"n_h" = n_h,
"n_y" = n_y)
return(size)
}
#### Aplica a função de criação de arquitetura
layer_size <- getLayerSize(train_x, train_y, hidden_neurons = 4)
layer_size
scale(data)
getLayerSize <- function(X, y, hidden_neurons) {
n_x <- dim(X)[1]      # quantidade de linhas de x = neurônios da camada de entrada
n_h <- hidden_neurons # quantidade de neurônios na camada escondida
n_y <- dim(y)[1]      # quantidade de linhas de y = neurônios da camada de saída
size <- list("n_x" = n_x,
"n_h" = n_h,
"n_y" = n_y)
return(size)
}
#### Aplica a função de criação de arquitetura
layer_size <- getLayerSize(train_x, train_y, hidden_neurons = 4)
layer_size
initializeParameters <- function(X, layer_size){
# arquitetura de rede
n_x <- layer_size$n_x # 14
n_h <- layer_size$n_h # 4
n_y <- layer_size$n_y # 1
W1 <- matrix(runif(n_h * n_x), nrow = n_h, ncol = n_x, byrow = TRUE) * 0.01
W2 <- matrix(runif(n_y * n_h), nrow = n_y, ncol = n_h, byrow = TRUE) * 0.01
params <- list("W1" = W1,
"W2" = W2)
return (params)
}
layer_size$n_x
n_x <- layer_size$n_x # 14
n_h <- layer_size$n_h # 4
n_y <- layer_size$n_y # 1
matrix(runif(n_h * n_x), nrow = n_h, ncol = n_x, byrow = TRUE)
matrix(runif(n_h * n_x), nrow = n_h, ncol = n_x, byrow = TRUE) * 0.01
initializeParameters <- function(X, layer_size){
# arquitetura de rede
n_x <- layer_size$n_x # 14
n_h <- layer_size$n_h # 4
n_y <- layer_size$n_y # 1
W1 <- matrix(runif(n_h * n_x), nrow = n_h, ncol = n_x, byrow = TRUE) * 0.01
W2 <- matrix(runif(n_y * n_h), nrow = n_y, ncol = n_h, byrow = TRUE) * 0.01
params <- list("W1" = W1,
"W2" = W2)
return (params)
}
#### Aplica a função de inicializar pesos.
init_params <- initializeParameters(train_x, layer_size)
lapply(init_params, function(x) dim(x))
###############################
#### Função 3 - Função Sigmoide
# Função de ativação utilziada ao longo do nosso trabalho. Iremos fazer o arco
# tangente também, mas isso já temos função no R: tanh().
sigmoid <- function(x){
return(1 / (1 + exp(-x)))
}
#### Aplica a função de inicializar pesos.
init_params <- initializeParameters(train_x, layer_size)
lapply(init_params, function(x) dim(x))
initializeParameters(train_x, layer_size)
###############################
#### Função 3 - Função Sigmoide
# Função de ativação utilziada ao longo do nosso trabalho. Iremos fazer o arco
# tangente também, mas isso já temos função no R: tanh().
sigmoid <- function(x){
return(1 / (1 + exp(-x)))
}
return(1 / (1 + exp(-x)))
###################################
#### Função 4 - Forward Propagation
# Agora com todas as informações dos pesos e estrutura da rede, iniciamos o
# processo de geração de resultados. A multiplicação de matrizes será feita por
# meio do operador %*%.
forwardPropagation <- function(X, params, layer_size){
n_h <- layer_size$n_h
n_y <- layer_size$n_y
W1 <- params$W1
W2 <- params$W2
Z1 <- W1 %*% X           # entre camada de entrada e camada escondida
A1 <- sigmoid(Z1)        # dentro da camanda escondida
Z2 <- W2 %*% A1          # entre camada escondida e camada de saída
A2 <- sigmoid(Z2)        # dentro da camada de saída (y)
cache <- list("Z1" = Z1,
"A1" = A1,
"Z2" = Z2,
"A2" = A2)
return (cache)
}
forwardPropagation(train_x, init_params, layer_size)
fwd_prop <- forwardPropagation(train_x, init_params, layer_size)
m <- dim(y)[2]
computeCost <- function(y, cache) {
m <- dim(y)[2]
A2 <- cache$A2
cost <- sum((y-A2)^2)/m
return (cost)
}
computeCost(train_y, fwd_prop)
#### Aplica a função custo
cost <- computeCost(train_y, fwd_prop)
backwardPropagation <- function(X, y, cache, params, layer_size){
m <- dim(X)[2]
n_x <- layer_size$n_x
n_h <- layer_size$n_h
n_y <- layer_size$n_y
A2 <- cache$A2
A1 <- cache$A1
W2 <- params$W2
dZ2 <- A2 - y
dW2 <- 1/m * (dZ2 %*% t(A1))
dZ1 <- (t(W2) %*% dZ2) * (1 - A1^2)
dW1 <- 1/m * (dZ1 %*% t(X))
grads <- list("dW1" = dW1,
"dW2" = dW2)
return(grads)
}
updateParameters <- function(grads, params, learning_rate){
W1 <- params$W1
W2 <- params$W2
dW1 <- grads$dW1
dW2 <- grads$dW2
W1 <- W1 - learning_rate * dW1
W2 <- W2 - learning_rate * dW2
updated_params <- list("W1" = W1,
"W2" = W2)
return (updated_params)
}
trainModel <- function(X, y, num_iteration, hidden_neurons, lr){
layer_size <- getLayerSize(X, y, hidden_neurons)
init_params <- initializeParameters(X, layer_size)
cost_history <- c()
for (i in 1:num_iteration) {
fwd_prop <- forwardPropagation(X, init_params, layer_size)
cost <- computeCost(y, fwd_prop)
back_prop <- backwardPropagation(X, y, fwd_prop, init_params, layer_size)
update_params <- updateParameters(back_prop, init_params, learning_rate = lr)
init_params <- update_params
cost_history <- c(cost_history, cost)
}
model_out <- list("updated_params" = update_params,
"cost_hist" = cost_history)
return (model_out)
}
#########################
#### Aplica o treinamento
EPOCHS = 100
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
########################
#### Testa os resultados
# Gera previsões
layer_size <- getLayerSize(test_x, test_y, HIDDEN_NEURONS)
params <- train_model$updated_params
fwd_prop <- forwardPropagation(test_x, params, layer_size)
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)
# Verifica função custo
plot(train_model$cost_hist)
cost
# Deep Learning - aula 1
# Modelando um algoritmo de Deep Learning na unha
pacotes <- c("plotly",
"tidyverse")
if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
instalador <- pacotes[!pacotes %in% installed.packages()]
for(i in 1:length(instalador)) {
install.packages(instalador, dependencies = T)
break()}
sapply(pacotes, require, character = T)
} else {
sapply(pacotes, require, character = T)
}
#### Garante que os resultados sejam reproduzíveis
set.seed(0)
#### Lê nossos dados de vinho
data <- read.csv(file = "winequality-red.csv")
data <- scale(data)
#### Estatísticas univariadas
summary(data)
#### Train Test Split (total de linhas = 1599 | 80% = 1279.2)
train_test_split_index <- 0.8 * nrow(data)
train_test_split_index # output: [1] 1279.2 (valor float)
# conjunto de treinamento
train <- data.frame(data[1:train_test_split_index,])
# conjunto de teste
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
#### Padronizar dados para melhor performance
train_x <- data.frame(train[1:11]) # variáveis explicativas (xs)
train_y <- data.frame(train[12])   # variável target (y)
test_x <- data.frame(test[1:11])   # variáveis explicativas (xs)
test_y <- data.frame(test[12])     # variável target (y)
#### transposição da matriz para facilitar
train_x <- t(train_x)
train_y <- t(train_y)
test_x <- t(test_x )
test_y <- t(test_y)
#########################################
#### Função 1 - Criar arquitetura da rede
getLayerSize <- function(X, y, hidden_neurons) {
n_x <- dim(X)[1]      # quantidade de linhas de x = neurônios da camada de entrada
n_h <- hidden_neurons # quantidade de neurônios na camada escondida
n_y <- dim(y)[1]      # quantidade de linhas de y = neurônios da camada de saída
size <- list("n_x" = n_x,
"n_h" = n_h,
"n_y" = n_y)
return(size)
}
#### Aplica a função de criação de arquitetura
layer_size <- getLayerSize(train_x, train_y, hidden_neurons = 4)
layer_size
####################################################
#### Função 2 - Inicializa Parâmetros randomicamente
# A partir da arquitetura, inicializamos os parâmetros
#randomicamente. O primeiro conjunto é W1 e b1. O segundo é
#W2 and b2.O valor destes parâmetros dependem do tamanho das
#camadas de entrada e saída.
initializeParameters <- function(X, layer_size){
# arquitetura de rede
n_x <- layer_size$n_x # 14
n_h <- layer_size$n_h # 4
n_y <- layer_size$n_y # 1
W1 <- matrix(runif(n_h * n_x), nrow = n_h, ncol = n_x, byrow = TRUE) * 0.01
W2 <- matrix(runif(n_y * n_h), nrow = n_y, ncol = n_h, byrow = TRUE) * 0.01
params <- list("W1" = W1,
"W2" = W2)
return (params)
}
#### Aplica a função de inicializar pesos.
init_params <- initializeParameters(train_x, layer_size)
lapply(init_params, function(x) dim(x))
###############################
#### Função 3 - Função Sigmoide
# Função de ativação utilziada ao longo do nosso trabalho. Iremos fazer o arco
# tangente também, mas isso já temos função no R: tanh().
sigmoid <- function(x){
return(1 / (1 + exp(-x)))
}
###################################
#### Função 4 - Forward Propagation
# Agora com todas as informações dos pesos e estrutura da rede, iniciamos o
# processo de geração de resultados. A multiplicação de matrizes será feita por
# meio do operador %*%.
forwardPropagation <- function(X, params, layer_size){
n_h <- layer_size$n_h
n_y <- layer_size$n_y
W1 <- params$W1
W2 <- params$W2
Z1 <- W1 %*% X           # entre camada de entrada e camada escondida
A1 <- sigmoid(Z1)        # dentro da camanda escondida
Z2 <- W2 %*% A1          # entre camada escondida e camada de saída
A2 <- sigmoid(Z2)        # dentro da camada de saída (y)
cache <- list("Z1" = Z1,
"A1" = A1,
"Z2" = Z2,
"A2" = A2)
return (cache)
}
###########################################
#### Aplica a função de forward propagation
fwd_prop <- forwardPropagation(train_x, init_params, layer_size)
############################
#### Função 5- Cost Function
# Mean Squared Error
computeCost <- function(y, cache) {
m <- dim(y)[2]
A2 <- cache$A2
cost <- sum((y-A2)^2)/m
return (cost)
}
#### Aplica a função custo
cost <- computeCost(train_y, fwd_prop)
###############################
#### Função 6 - Backpropagation
#### Essa parte exige conhecimento de cálculo diferencial para entender o
# gradiente. Entretanto, só irei dar a noção lógica do que está ocorrendo sem
# necessidade de adentrar no cálculo. Para mais detalhes vide:
# https://rpubs.com/theairbend3r/bin-classification-small-nnet-scratch-r.
backwardPropagation <- function(X, y, cache, params, layer_size){
m <- dim(X)[2]
n_x <- layer_size$n_x
n_h <- layer_size$n_h
n_y <- layer_size$n_y
A2 <- cache$A2
A1 <- cache$A1
W2 <- params$W2
dZ2 <- A2 - y
dW2 <- 1/m * (dZ2 %*% t(A1))
dZ1 <- (t(W2) %*% dZ2) * (1 - A1^2)
dW1 <- 1/m * (dZ1 %*% t(X))
grads <- list("dW1" = dW1,
"dW2" = dW2)
return(grads)
}
##############################
#### Função 7 - Atualiza pesos
#### A atualização de peso é feita com base nos cálculos anteriores e na taxa
# de aprendizagem.
updateParameters <- function(grads, params, learning_rate){
W1 <- params$W1
W2 <- params$W2
dW1 <- grads$dW1
dW2 <- grads$dW2
W1 <- W1 - learning_rate * dW1
W2 <- W2 - learning_rate * dW2
updated_params <- list("W1" = W1,
"W2" = W2)
return (updated_params)
}
#########################
# Parte 9 - Treina modelo
#### Quais as etapas?
#- Arquitetura da rede.
#- Inicializa um vetor de historia da função custo para guardar resultados.
#- Faz foward loop.
#- Calcula perda.
#- Atualiza parâmetros.
#- Usa os novos parâmetros.
##### Entra o conceitos de épocas (EPOCHS).
trainModel <- function(X, y, num_iteration, hidden_neurons, lr){
layer_size <- getLayerSize(X, y, hidden_neurons)
init_params <- initializeParameters(X, layer_size)
cost_history <- c() # cria um vetor vazio
for (i in 1:num_iteration) {
fwd_prop <- forwardPropagation(X, init_params, layer_size)
cost <- computeCost(y, fwd_prop)
back_prop <- backwardPropagation(X, y, fwd_prop, init_params, layer_size)
update_params <- updateParameters(back_prop, init_params, learning_rate = lr)
init_params <- update_params
cost_history <- c(cost_history, cost)
}
model_out <- list("updated_params" = update_params,
"cost_hist" = cost_history)
return (model_out)
}
#########################
#### Aplica o treinamento
EPOCHS = 100
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
########################
#### Testa os resultados
# Gera previsões
layer_size <- getLayerSize(test_x, test_y, HIDDEN_NEURONS)
params <- train_model$updated_params
fwd_prop <- forwardPropagation(test_x, params, layer_size)
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)
# Verifica função custo
plot(train_model$cost_hist)
#########################
#### Aplica o treinamento
EPOCHS = 100
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
########################
#### Testa os resultados
# Gera previsões
layer_size <- getLayerSize(test_x, test_y, HIDDEN_NEURONS)
params <- train_model$updated_params
fwd_prop <- forwardPropagation(test_x, params, layer_size)
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)
# Verifica função custo
plot(train_model$cost_hist)
#########################
#### Aplica o treinamento
EPOCHS = 100
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
########################
#### Testa os resultados
# Gera previsões
layer_size <- getLayerSize(test_x, test_y, HIDDEN_NEURONS)
params <- train_model$updated_params
fwd_prop <- forwardPropagation(test_x, params, layer_size)
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)
# Verifica função custo
plot(train_model$cost_hist)
#########################
#### Aplica o treinamento
EPOCHS = 100
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
########################
#### Testa os resultados
# Gera previsões
layer_size <- getLayerSize(test_x, test_y, HIDDEN_NEURONS)
params <- train_model$updated_params
fwd_prop <- forwardPropagation(test_x, params, layer_size)
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)
# Verifica função custo
plot(train_model$cost_hist)
compare
View(compare)
View(compare)
