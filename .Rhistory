# de aprendizagem.
updateParameters <- function(grads, params, learning_rate){
W1 <- params$W1
W2 <- params$W2
dW1 <- grads$dW1
dW2 <- grads$dW2
W1 <- W1 - learning_rate * dW1
W2 <- W2 - learning_rate * dW2
updated_params <- list("W1" = W1,
"W2" = W2)
return (updated_params)
}
#########################
# Parte 9 - Treina modelo
# Quais as etapas?
# - Arquitetura da rede.
# - Inicializa um vetor de historia da função custo para guardar resultados.
# - Faz foward loop.
# - Calcula perda.
# - Atualiza parâmetros.
# - Usa os novos parâmetros.
##### Entra o conceitos de épocas (EPOCHS).
trainModel <- function(X, y, num_iteration, hidden_neurons, lr){
layer_size <- getLayerSize(X, y, hidden_neurons)
init_params <- initializeParameters(X, layer_size)
cost_history <- c() # cria um vetor vazio
for (i in 1:num_iteration) {
fwd_prop <- forwardPropagation(X, init_params, layer_size)
cost <- computeCost(y, fwd_prop)
back_prop <- backwardPropagation(X, y, fwd_prop, init_params, layer_size)
update_params <- updateParameters(back_prop, init_params, learning_rate = lr)
init_params <- update_params
cost_history <- c(cost_history, cost)
}
model_out <- list("updated_params" = update_params,
"cost_hist" = cost_history)
return (model_out)
}
#########################
#### Aplica o treinamento
EPOCHS = 100
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
########################
#### Testa os resultados
# Gera previsões
layer_size <- getLayerSize(test_x, test_y, HIDDEN_NEURONS)
params <- trfwd_prop$ain_model$updated_params
fwd_prop <- forwardPropagation(test_x, params, layer_size)
y_pred <- A2
compare <- rbind(y_pred,test_y)
computeCost(train_y, fwd_prop)
#### Aplica a função custo
cost <- computeCost(train_y, fwd_prop)
backwardPropagation <- function(X, y, cache, params, layer_size){
m <- dim(X)[2]
n_x <- layer_size$n_x
n_h <- layer_size$n_h
n_y <- layer_size$n_y
A2 <- cache$A2
A1 <- cache$A1
W2 <- params$W2
# dZ2 erro
# A2 valor predito - Y valor real
dZ2 <- A2 - y
# dw2 quanto temos que mexer no parametro para reajustar
# baseado em calculo diferencial
dW2 <- 1/m * (dZ2 %*% t(A1))
dZ1 <- (t(W2) %*% dZ2) * (1 - A1^2)
dW1 <- 1/m * (dZ1 %*% t(X))
grads <- list("dW1" = dW1,
"dW2" = dW2)
return(grads)
}
updateParameters <- function(grads, params, learning_rate){
W1 <- params$W1
W2 <- params$W2
dW1 <- grads$dW1
dW2 <- grads$dW2
W1 <- W1 - learning_rate * dW1
W2 <- W2 - learning_rate * dW2
updated_params <- list("W1" = W1,
"W2" = W2)
return (updated_params)
}
trainModel <- function(X, y, num_iteration, hidden_neurons, lr){
layer_size <- getLayerSize(X, y, hidden_neurons)
init_params <- initializeParameters(X, layer_size)
cost_history <- c() # cria um vetor vazio
for (i in 1:num_iteration) {
fwd_prop <- forwardPropagation(X, init_params, layer_size)
cost <- computeCost(y, fwd_prop)
back_prop <- backwardPropagation(X, y, fwd_prop, init_params, layer_size)
update_params <- updateParameters(back_prop, init_params, learning_rate = lr)
init_params <- update_params
cost_history <- c(cost_history, cost)
}
model_out <- list("updated_params" = update_params,
"cost_hist" = cost_history)
return (model_out)
}
#########################
#### Aplica o treinamento
EPOCHS = 100
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
compare <- rbind(y_pred,test_y)
y_pred <- A2
fwd_prop$A2
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)
########################
#### Testa os resultados
# Gera previsões
layer_size <- getLayerSize(test_x, test_y, HIDDEN_NEURONS)
params <- trfwd_prop$ain_model$updated_params
fwd_prop$ain_model$updated_params
trfwd_prop
train_model$updated_params
params <- train_model$updated_params
fwd_prop <- forwardPropagation(test_x, params, layer_size)
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)
compare
train_model$cost_hist
train_model$cost_hist
# Verifica função custo
plot(train_model$cost_hist)
forwardPropagation(train_x, init_params, layer_size)
computeCost <- function(y, cache) {
m <- dim(y)[2] #dim(train_y)[2]
A2 <- cache$A2 #camada de saída (y)
# soma do valor ao quadrado do erro (y - A2)
# divido pela quantidade de observações
cost <- sum((y-A2)^2)/m
return (cost)
}
#### Aplica a função custo
cost <- computeCost(train_y, fwd_prop)
computeCost(train_y, fwd_prop)
fwd_prop <- forwardPropagation(train_x, init_params, layer_size)
computeCost(train_y, fwd_prop)
train_model$cost_hist
##################################################################################
#                  INSTALAÇÃO E CARREGAMENTO DE PACOTES NECESSÁRIOS             #
##################################################################################
#Pacotes utilizados
pacotes <- c("MASS",
"neuralnet",
"ISLR",
"mlbench",
"neuralnet",
"rpart")
if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
instalador <- pacotes[!pacotes %in% installed.packages()]
for(i in 1:length(instalador)) {
install.packages(instalador, dependencies = T)
break()}
sapply(pacotes, require, character = T)
} else {
sapply(pacotes, require, character = T)
}
#install.packages("MASS")
library("MASS")
library("rpart")
set.seed(0)
#Baixa os dados
data <- Boston
data
#Uma olhada nos dados
head(data)
data[is.na(data) == TRUE]
0.8 * nrow(data)
# Train-Test Split
train_test_split_index <- 0.8 * nrow(data)
data.frame(data[1:train_test_split_index,])
train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
fit_tree <- rpart(medv ~.,method="anova", data=train)
fit_tree
predict(fit_tree,test)
tree_predict <- predict(fit_tree,test)
test$medv
mean((tree_predict - test$medv)^2)
mse_tree <- mean((tree_predict - test$medv)^2)
mse_tree
cat("MSE: ")
mse_tree
cat("MSE: ", mse_tree)
cat("MSE:", mse_tree)
View(data)
?rpart
fit_tree
fit_tree$frame
fit_tree$frame
fit_tree$y
# install.packages("MASS")
library("MASS")
library("rpart")
set.seed(0)
# Baixa os dados
data <- Boston
View(data)
# Uma olhada nos dados
head(data)
# Temos valores nulos?
data[is.na(data) == TRUE]
# Train-Test Split
train_test_split_index <- 0.8 * nrow(data)
train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
fit_tree <- rpart(medv ~., method = "anova", data=train) #anova - regressão
tree_predict <- predict(fit_tree,test)
mse_tree <- mean((tree_predict - test$medv)^2)
cat("MSE:", mse_tree)
# Conceito de função
# y = f(x)
# y = a +b1*x1 +b2*x2...
# install.packages("MASS")
library("MASS")
library("rpart")
set.seed(0)
# Baixa os dados
data <- Boston
# Uma olhada nos dados
head(data)
# Temos valores nulos?
data[is.na(data) == TRUE]
# Train-Test Split
train_test_split_index <- 0.8 * nrow(data)
train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
# --
# CART
# árvore
# ?rpart - Recursive Partitioning and Regression Trees
fit_tree <- rpart(medv ~., method = "anova", data=train) #anova - regressão
tree_predict <- predict(fit_tree,test)
mse_tree <- mean((tree_predict - test$medv)^2)
cat("MSE:", mse_tree)
fit_tree$frame
# Conceito de função
# y = f(x)
# y = a +b1*x1 +b2*x2...
# install.packages("MASS")
library("MASS")
library("rpart")
set.seed(0)
# Baixa os dados
data <- Boston
# Uma olhada nos dados
head(data)
# Temos valores nulos?
data[is.na(data) == TRUE]
# Train-Test Split
train_test_split_index <- 0.8 * nrow(data)
train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
# --
# CART
# árvore
# ?rpart - Recursive Partitioning and Regression Trees
fit_tree <- rpart(medv ~., method = "anova", data=train) #anova - regressão
tree_predict <- predict(fit_tree,test)
mse_tree <- mean((tree_predict - test$medv)^2)
cat("MSE:", mse_tree)
fit_tree$frame
# --
# CART
# árvore
# ?rpart - Recursive Partitioning and Regression Trees
fit_tree <- rpart(medv ~., method = "anova", data=train) # anova - regressão (definido pelo tipo da variável Y)
tree_predict <- predict(fit_tree, test)                  # predict função do pacote statis
mse_tree <- mean((tree_predict - test$medv) ^ 2)         # calcular o Erro Quadrático Médio "MSE"
cat("MSE:", mse_tree)
fit_tree$frame
# Padronizar dados para melhor performance
# Explicar apply
set.seed(0)
max_data <- apply(data, 2, max)
max_data
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- scale(data,center = min_data, scale = max_data - min_data)
sample(1:nrow(data),round(0.70*nrow(data)))
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
library(neuralnet)
# abrir o CRAN para mostrar
# Fit de neuralnet
# Executar testes com diferentes arquiteturas
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,data=train_data,hidden=c(5,4,3,2),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE_nn <- mean((pr.nn_ - test.r)^2)
cat("MSE:", MSE_nn)
library(neuralnet)
# abrir o CRAN para mostrar
# Fit de neuralnet
# Executar testes com diferentes arquiteturas
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,data=train_data,hidden=c(5,4,3,2),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE_nn <- mean((pr.nn_ - test.r)^2)
cat("MSE:", MSE_nn)
plot(test_data$medv,type = 'l',col="red",xlab = "x", ylab = "Valor Residencia")
lines(pr.nn$net.result,col = "blue")
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,data=train_data,hidden=c(5,4,3,2),linear.output=T)
plot(nn)
library(ggplot2)
# Plot do modelo neuralnet com ggplot2
plot_neuralnet <- function(model) {
# Extrair os pesos da rede neural
weights <- model$result$weights
# Criação do data.frame com os pesos e conexões
df <- data.frame(
from = rep(1:length(weights), each = length(weights[[1]])),
to = 1:length(weights[[1]]),
weight = unlist(weights)
)
# Plot da rede neural usando ggplot2
ggplot(df, aes(x = from, y = to, size = abs(weight), color = weight)) +
geom_point() +
scale_size_continuous(range = c(0.5, 5)) +
scale_color_gradient2(low = "blue", high = "red", mid = "white") +
theme_minimal() +
labs(title = "Gráfico da Rede Neural", x = "Nó de Entrada", y = "Nó de Saída") +
coord_fixed()  # Para manter proporções iguais entre eixos X e Y
}
# Chamada da função para plotar a rede neural criada
plot_neuralnet(nn)
library(ggplot2)
# Plot do modelo neuralnet com ggplot2
plot_neuralnet <- function(model) {
# Extrair os pesos da rede neural
weights <- model$result$weights
# Criação do data.frame com os pesos e conexões
df <- data.frame(
from = rep(1:length(weights), each = length(weights[[1]])),
to = 1:length(weights[[1]]),
weight = unlist(weights)
)
# Plot da rede neural usando ggplot2
ggplot(df, aes(x = from, y = to, size = abs(weight), color = weight)) +
geom_point() +
scale_size_continuous(range = c(0.5, 5)) +
scale_color_gradient2(low = "blue", high = "red", mid = "white") +
theme_minimal() +
labs(title = "Gráfico da Rede Neural", x = "Nó de Entrada", y = "Nó de Saída") +
coord_fixed()  # Para manter proporções iguais entre eixos X e Y
}
# Chamada da função para plotar a rede neural criada
plot_neuralnet(nn)
nn$result$weights
nn$weights
library(ggplot2)
# Plot do modelo neuralnet com ggplot2
plot_neuralnet <- function(model) {
# Extrair os pesos da rede neural
weights <- model$weights
# Criação do data.frame com os pesos e conexões
df <- data.frame(
from = rep(1:length(weights), each = length(weights[[1]])),
to = 1:length(weights[[1]]),
weight = unlist(weights)
)
# Plot da rede neural usando ggplot2
ggplot(df, aes(x = from, y = to, size = abs(weight), color = weight)) +
geom_point() +
scale_size_continuous(range = c(0.5, 5)) +
scale_color_gradient2(low = "blue", high = "red", mid = "white") +
theme_minimal() +
labs(title = "Gráfico da Rede Neural", x = "Nó de Entrada", y = "Nó de Saída") +
coord_fixed()  # Para manter proporções iguais entre eixos X e Y
}
# Chamada da função para plotar a rede neural criada
plot_neuralnet(nn)
library(neuralnet)
# abrir o CRAN para mostrar
# Fit de neuralnet
# Executar testes com diferentes arquiteturas
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,data=train_data,hidden=c(5,4,3,2),linear.output=T)
plot(nn, par(mar = c(5, 4, 4, 2)))
library(neuralnet)
# abrir o CRAN para mostrar
# Fit de neuralnet
# Executar testes com diferentes arquiteturas
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,data=train_data,hidden=c(5,4,3,2),linear.output=T)
plot(nn)
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,data=train_data,hidden=c(5,4,3,2),linear.output=T)
plot(nn)
?sample
?neuralnet
library(neuralnet)
# abrir o CRAN para mostrar
# Fit de neuralnet
# Executar testes com diferentes arquiteturas
# hidden -> numero de camadas e neuronios
#?neuralnet
nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat,data=train_data,hidden=c(5,4,3,2),linear.output=T)
plot(nn)
plot(nn, rep = NULL, x.entry = NULL, x.out = NULL,radius = 0.15, arrow.length = 0.2, intercept = TRUE,
intercept.factor = 0.4, information = TRUE, information.pos = 0.1,
col.entry.synapse = "black", col.entry = "black",
col.hidden = "black", col.hidden.synapse = "black",
col.out = "black", col.out.synapse = "black",
col.intercept = "blue", fontsize = 12, dimension = 6,
show.weights = TRUE, file = NUL)
plot(nn, rep = NULL, radius = 0.15, arrow.length = 0.2, intercept = TRUE,
intercept.factor = 0.4, information = TRUE, information.pos = 0.1,
col.entry.synapse = "black", col.entry = "black",
col.hidden = "black", col.hidden.synapse = "black",
col.out = "black", col.out.synapse = "black",
col.intercept = "blue", fontsize = 12, dimension = 6,
show.weights = TRUE, file = NUL)
plot(nn,radius = 0.15, arrow.length = 0.2, intercept = TRUE,
intercept.factor = 0.4, information = TRUE, information.pos = 0.1,
col.entry.synapse = "black", col.entry = "black",
col.hidden = "black", col.hidden.synapse = "black",
col.out = "black", col.out.synapse = "black",
col.intercept = "blue", fontsize = 12, dimension = 6,
show.weights = TRUE, file = NUL)
plot(nn,show.weights = TRUE, file = NUL)
plot(nn)
plot(nn)
plot(nn,rep="best")
plot(nn.xor,rep="best")
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE_nn <- mean((pr.nn_ - test.r)^2)
cat("MSE:", MSE_nn)
# voltar com valor não normalizado para comparação
pr.nn <- compute(nn,test_data[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE_nn <- mean((pr.nn_ - test.r)^2)
cat("MSE:", MSE_nn)
plot(test_data$medv,type = 'l',col="red",xlab = "x", ylab = "Valor Residencia")
lines(pr.nn$net.result,col = "blue")
plot(test_data$medv,type = 'l',col="red",xlab = "x", ylab = "Valor Residencia")
lines(pr.nn$net.result,col = "blue")
library(ISLR)
library(neuralnet)
# Olhar os dados - data wrangling?
data <- College
# is.na(data)
View(data)
# is.na(data)
head(data)
ifelse(data$Private == 'Yes', 1, 0)
# private = as.numeric(College$Private)-1
private <- ifelse(data$Private == 'Yes', 1, 0)
# Padronizar dados para melhor performance
data <- data[,2:18]
max_data <- apply(data, 2, max)
min_data <- apply(data, 2, min)
scaled <- data.frame(scale(data,center = min_data, scale = max_data - min_data))
# Inclui variável explicada (target)
scaled$Private <- private
set.seed(0)
# train test split
index = sample(1:nrow(data),round(0.70*nrow(data)))
train_data <- as.data.frame(scaled[index,])
test_data <- as.data.frame(scaled[-index,])
# Utiliza o neuralnet
set.seed(0)
n = names(train_data)
f <- as.formula(paste("Private ~", paste(n[!n %in% "Private"], collapse = " + ")))
nn <- neuralnet(f,data=train_data,hidden=c(5,4),linear.output=F)
plot(nn)
pr.nn <- compute(nn,test_data[,1:17])
# Explica sapply
pr.nn$net.result <- sapply(pr.nn$net.result,round,digits=0)
pr.nn$net.result
table(test_data$Private,pr.nn$net.result)
Acc <- (62+158) / (62+158+7+6)
# CART comparação
set.seed(0)
# árvore
fit_tree <- rpart(f,method="class", data=train_data)
tree_predict <- predict(fit_tree,test_data,type = "class")
table(test_data$Private,tree_predict)
Acc_tree <- (58+159) / (58+159+11+5)
table(test_data$Private, tree_predict)
table(test_data$Private,pr.nn$net.result)
Acc <- (61+152) / (61+152+8+12)
# Explica sapply
pr.nn$net.result <- sapply(pr.nn$net.result, round, digits=0)
pr.nn$net.result
table(test_data$Private, pr.nn$net.result)
Acc <- (61+152) / (61+152+8+12)
# Explica sapply
pr.nn$net.result <- sapply(pr.nn$net.result, round, digits=0)
#pr.nn$net.result
table(test_data$Private, pr.nn$net.result)
Acc <- (61+152) / (61+152+8+12)
table(test_data$Private, tree_predict)
# CART comparação
set.seed(0)
# árvore
fit_tree <- rpart(f,method="class", data=train_data)
tree_predict <- predict(fit_tree, test_data, type = "class")
table(test_data$Private, tree_predict)
Acc_tree <- (58+159) / (58+159+11+5)
